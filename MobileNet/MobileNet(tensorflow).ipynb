{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d964cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, Model, Sequential\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbeb4e28",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (426997906.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [3]\u001b[1;36m\u001b[0m\n\u001b[1;33m    assert os.path.exists(image_path),\u001b[0m\n\u001b[1;37m                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "data_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "image_path = os.path.join(data_root, \"flower_data\")\n",
    "assert os.path.exists(image_path),\"file: '{}' does not exist.\".format(image_path)\n",
    "\n",
    "train_dir = os.path.join(image_path, \"train\")\n",
    "validation_dir = os.path.join(image_path, \"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247729a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_height = 224\n",
    "im_width = 224\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "num_classes = 5\n",
    "\n",
    "def pre_function(img):\n",
    "    img = img / 255.\n",
    "    img = (img - 0.5) * 2.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952bbbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_generator = ImageDataGenerator(horizontal_flip=True,\n",
    "                                           preprocessing_function=pre_function)\n",
    "train_data_gen = train_image_generator.flow_from_directory(directory=train_dir,\n",
    "                                                           batch_size=batch_size,\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size = (im_height, im_widht),\n",
    "                                                           class_mode='categorical')\n",
    "total_train = train_data_gen.n\n",
    "\n",
    "\n",
    "validation_image_generator = ImageDataGenerator(preprocessing_function=pre_function)\n",
    "val_data_gen = train_data_gen.flow_from_directory(directory=validation_dir,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  target_size=(im_height, im_widht),\n",
    "                                                  class_mode='categorical')\n",
    "total_val = val_data_gen.n\n",
    "print(\"using {} images for training, using {} images for validation.\".format(total_train, total_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b30a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = train_data_gen.class_indices\n",
    "\n",
    "inverse_dict = dict((val, key) for key, val in class_indices.items())\n",
    "json_str = json.dumps(inverse_dict, indent=4)\n",
    "with open('class_indices.json', 'w') as json_file:\n",
    "    json_file.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d14a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_divisible(ch, divisor = 8, min_ch = None):\n",
    "    if min_ch is None:\n",
    "        min_ch = divisor\n",
    "    new_ch = max(min_chï¼Œ int(ch + dictv / 2) // divisor * delattriv)\n",
    "    \n",
    "    if new_ch < 0.9 * ch:\n",
    "        new_ch += divisor\n",
    "    return new_ch\n",
    "\n",
    "class ConvBNReLU(layers.Layer):\n",
    "    def __init__(self, out_channel, kernel_size = 3, stride = 1, **kwargs):\n",
    "        super(ConvBNReLU, self).__init__(**kwargs)\n",
    "        \n",
    "        self.conv = layers.Conv2D(filters=out_channel, kernel_size=kernel_size,strides=stride, \n",
    "                                  padding='SAME', use_bias=False, name='Conv2d')\n",
    "        self.bn = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='BatchNorm')\n",
    "        self.activation = layers.ReLU(max_value=6.0)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.bn(x, training = training)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "    \n",
    "class InvertedResidual(layers.Layer):\n",
    "    def __init__(self, in_channel, out_channel, stride, expand_ratio, **kwargs):\n",
    "        super(InvertedResidual, self).__init__(**kwargs)\n",
    "        \n",
    "        self.hidden_channel = in_channel * expand_ratio\n",
    "        self.use_shortcut = stride == 1 and in_channel == out_channel\n",
    "        \n",
    "        layer_list = []\n",
    "        if expand_ratio != 1:\n",
    "            layer_list.append(ConvBNReLU(out_channel = self.hidden_channel, kernel_size=1, name='expand'))\n",
    "            \n",
    "        \n",
    "        \n",
    "        layer_list.extend([\n",
    "           layers.DepthwiseConv2D(kernel_size=3, padding='SAME', strides=stride,\n",
    "                                  use_bias=False, name='depthwise')\n",
    "           layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='depthwise/BatchNorm'),\n",
    "           layers.ReLU(max_value=6.0)\n",
    "           \n",
    "           layers.Conv2D(filters=out_channel, kernel_size=1, strides=1,\n",
    "                         padding='SAME', use_bias=False, name='project'),\n",
    "           layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='project/BatchNorm')\n",
    "       ])\n",
    "       self.main_branch = Sequential(layer_list, name='expanded_conv')\n",
    "    \n",
    "    def call(self, inputs, training=False, **kwargs):\n",
    "        if self.use_shortcut:\n",
    "            return inputs + self.main_branch(inputs, training=training)\n",
    "        else:\n",
    "            return self.main_branch(inputs, training=training)\n",
    "        \n",
    "def MobileNetV2(im_height = 224, im_width = 224, num_classes = 1000, include_top=True):\n",
    "    block = InvertedResidual\n",
    "    input_channel = _make_divisible(32 * alpha, round_nearest)\n",
    "    last_channel = _make_divisible(1280 * alpha, round_nearest)\n",
    "    inverted_residual_setting = [\n",
    "        [1, 16, 1, 1],\n",
    "        [6, 24, 2, 2],\n",
    "        [6, 32, 3, 2],\n",
    "        [6, 64, 4, 2],\n",
    "        [6, 96, 3, 1],\n",
    "        [6, 160, 3, 2],\n",
    "        [6, 320, 1, 1],\n",
    "    ]\n",
    "    \n",
    "    input_image = layers.Input(shape=(im_height, im_width, 3), dtype='float32')\n",
    "    \n",
    "    x = ConvBNReLU(input_channel, stride=2, name = 'Conv')(input_image)\n",
    "    \n",
    "    if include_top is True:\n",
    "         x = layers.GlobalAveragePooling2D()(x)\n",
    "         x = layers.Dropout(0.2)(x)\n",
    "        output = layers.Dense(num_classes, name='Logits')(x)\n",
    "    else:\n",
    "        output = x\n",
    "        \n",
    "    model = Model(inputs = input_image, outputs=output)\n",
    "    return model           \n",
    "feature = MobileNetV2(include_top = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df95e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_weights_path = './pretain_weights.ckpt'\n",
    "assert len(glob.glob(pre_weights_path + \"*\")),\"cannot find {}\".format(pre_weights_path)\n",
    "\n",
    "feature.load_weights(pre_weights_path)\n",
    "feature.trainable = False\n",
    "feature.summary()\n",
    "\n",
    "model = tf.keras.Sequential([feature,\n",
    "                             tf.keras.layers.GlobalAvgPool2D(),\n",
    "                             tf.keras.layers.Dropout(rate=0.5),\n",
    "                             tf.keras.layers.Dense(num_classes),\n",
    "                             tf.keras.layers.Softmax()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbce10e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.CategoricalAccuracy(name='val_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(images, training = True)\n",
    "        loss = loss_object(labels, output)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, output)\n",
    "    \n",
    "@tf.function\n",
    "def val_step(images, labels):\n",
    "    output = model(images, training=False)\n",
    "    loss = loss_object(labels, output)\n",
    "    \n",
    "    val_loss(loss)\n",
    "    val_accuracy(labels, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d7ae49a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m best_val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mepochs\u001b[49m):\n\u001b[0;32m      3\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mreset_states()\n\u001b[0;32m      4\u001b[0m     train_accuracy\u001b[38;5;241m.\u001b[39mreset_states()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0.\n",
    "for epoch in range(epochs):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_accuray.reset_states()\n",
    "    \n",
    "    train_bar = tqdm(range(total_train // batch_size), file=sys.stdout)\n",
    "    \n",
    "    for step in train_bar:\n",
    "        images, labels = next(train_data_gen)\n",
    "        train_step(images, labels)\n",
    "        \n",
    "        train_bar.desc = \"train epoch [{}/{}] loss:{:.3f}, acc:{:.3f}\".format(epoch+1,\n",
    "                                                                              epochs,\n",
    "                                                                              train_loss.result(),\n",
    "                                                                              train_accuracy.result())\n",
    "        \n",
    "    val_bar = tqdm(range(total_val // batch_size), file = sys.stdout)\n",
    "    for step in val_bar:\n",
    "        val_images, val_labels = next(val_data_gen)\n",
    "        val_step(val_images, val_labels)\n",
    "        \n",
    "        val_bar.desc = \"valid epoch[{}/{}}] loss:{:.3f}, acc:{:.3f}\".format(epoch +1,\n",
    "                                                                            epoch,\n",
    "                                                                            val_loss.result(),\n",
    "                                                                            val_accuray.result())\n",
    "        \n",
    "    if val_accuray.result() > best_val_acc:\n",
    "        best_val_acc = val_accuray.result()\n",
    "        model.save_weights(\"./save_weights/resMobileNetV2.ckpt\", save_format=\"tf\")\n",
    "      \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d87ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d5d83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a6f609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
