{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if os.path.exists(\"./weights\") is False:\n",
    "    os.makedirs(\"./weights\")\n",
    "\n",
    "tb_writer = SummaryWriter()\n",
    "\n",
    "img_size = 224\n",
    "batch_size = 8\n",
    "num_classes = 5\n",
    "epochs = 10\n",
    "lr = 0.0001\n",
    "\n",
    "data_root = './flower_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "size1 = [0.485, 0.456, 0.406]\n",
    "size2 = [0.229, 0.224, 0.225]\n",
    "\n",
    "data_transform = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomResizedCrop(img_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(size1, size2)\n",
    "    ]),\n",
    "    \"val\":transforms.Compose([\n",
    "        transforms.Resize(int(img_size * 1.145)),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(size1, size2)\n",
    "    ])\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 dataloader workers every process\n"
     ]
    }
   ],
   "source": [
    "nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])\n",
    "print(\"Using {} dataloader workers every process\".format(nw))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_dir = os.path.join(data_root, \"train\")\n",
    "val_dir = os.path.join(data_root, \"val\")\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=data_transform[\"train\"])\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=data_transform[\"train\"])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=nw)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                         batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         pin_memory=True,\n",
    "                                         num_workers=nw)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def drop_path_f(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "       return drop_path_f(x, self.drop_prob, self.training)\n",
    "\n",
    "def window_partition(x, window_size: int):\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows, window_size: int, H: int, W: int):\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, patch_size = 4, in_c = 3, embed_dim = 96, norm_layer = None):\n",
    "        super().__init__()\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_c\n",
    "        self.embed_dim = embed_dim\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, H, W = x.shape\n",
    "\n",
    "        pad_input = (H % self.patch_size[0] != 0) or (W % self.patch_size[1] != 0)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1],\n",
    "                          0, self.patch_size[0] - H % self.patch_size[0],\n",
    "                          0, 0))\n",
    "\n",
    "        x = self.proj(x)\n",
    "        _, _, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x, H, W\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias = False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W,\"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]\n",
    "        x1 = x[:, 1::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, :]\n",
    "        x3 = x[:, 1::2, 1::2, :]\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)\n",
    "        x = x.view(B, -1, 4 * C)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features = None, out_features = None, act_layer = nn.GELU, drop = 0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.drop1 = nn.Dropout(drop)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop2 = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias = True, attn_drop = 0., proj_drop = 0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n",
    "\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias = qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask: Optional[torch.Tensor] = None):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ //nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size = 7, shift_size = 0,\n",
    "                 mlp_ratio = 4., qkv_bias = True, drop = 0., attn_drop = 0., drop_path = 0.,\n",
    "                 act_layer = nn.GELU, norm_layer = nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        assert 0 <= self.shift_size < self.window_size,\"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=(self.window_size, self.window_size), num_heads = num_heads, qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        H, W = self.H, self.W\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W,\"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        pad_l = pad_t = 0\n",
    "        pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "        pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "\n",
    "        x_windows = window_partition(shifted_x, self.window_size)\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
    "\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)\n",
    "\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        if pad_r > 0 or pad_b > 0:\n",
    "            x = x[:, :H, :W, :].contiguous()\n",
    "\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    def __init__(self, dim, depth, num_heads, window_size,\n",
    "                 mlp_ratio = 4., qkv_bias = True, drop = 0., attn_drop = 0.,\n",
    "                 drop_path = 0., norm_layer = nn.LayerNorm, downsample = None, use_checkpoint = False):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.window_size = window_size\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.shift_size = window_size // 2\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else self.shift_size,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop,\n",
    "                attn_drop=attn_drop,\n",
    "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def create_mask(self, x, H, W):\n",
    "        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n",
    "        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n",
    "        # 拥有和feature map一样的通道排列顺序，方便后续window_partition\n",
    "        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # [1, Hp, Wp, 1]\n",
    "        h_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size)  # [nW, Mh, Mw, 1]\n",
    "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)  # [nW, Mh*Mw]\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)  # [nW, 1, Mh*Mw] - [nW, Mh*Mw, 1]\n",
    "        # [nW, Mh*Mw, Mh*Mw]\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        return attn_mask\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        attn_mask = self.create_mask(x, H, W)  # [nW, Mh*Mw, Mh*Mw]\n",
    "        for blk in self.blocks:\n",
    "            blk.H, blk.W = H, W\n",
    "            if not torch.jit.is_scripting() and self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x, attn_mask)\n",
    "            else:\n",
    "                x = blk(x, attn_mask)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x, H, W)\n",
    "            H, W = (H + 1) // 2, (W + 1) // 2\n",
    "\n",
    "        return x, H, W\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    r\"\"\" Swin Transformer\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "    Args:\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patch_size=4, in_chans=3, num_classes=1000,\n",
    "                 embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24),\n",
    "                 window_size=7, mlp_ratio=4., qkv_bias=True,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, patch_norm=True,\n",
    "                 use_checkpoint=False, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_norm = patch_norm\n",
    "        # stage4输出特征矩阵的channels\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            patch_size=patch_size, in_c=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            # 注意这里构建的stage和论文图中有些差异\n",
    "            # 这里的stage不包含该stage的patch_merging层，包含的是下个stage的\n",
    "            layers = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                                depth=depths[i_layer],\n",
    "                                num_heads=num_heads[i_layer],\n",
    "                                window_size=window_size,\n",
    "                                mlp_ratio=self.mlp_ratio,\n",
    "                                qkv_bias=qkv_bias,\n",
    "                                drop=drop_rate,\n",
    "                                attn_drop=attn_drop_rate,\n",
    "                                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                                norm_layer=norm_layer,\n",
    "                                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                                use_checkpoint=use_checkpoint)\n",
    "            self.layers.append(layers)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, L, C]\n",
    "        x, H, W = self.patch_embed(x)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x, H, W = layer(x, H, W)\n",
    "\n",
    "        x = self.norm(x)  # [B, L, C]\n",
    "        x = self.avgpool(x.transpose(1, 2))  # [B, C, 1]\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "def swin_tiny_patch4_window7_224(num_classes: int = 1000, **kwargs):\n",
    "    # trained ImageNet-1K\n",
    "    # https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth\n",
    "    model = SwinTransformer(in_chans=3,\n",
    "                            patch_size=4,\n",
    "                            window_size=7,\n",
    "                            embed_dim=96,\n",
    "                            depths=(2, 2, 6, 2),\n",
    "                            num_heads=(3, 6, 12, 24),\n",
    "                            num_classes=num_classes,\n",
    "                            **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def swin_small_patch4_window7_224(num_classes: int = 1000, **kwargs):\n",
    "    # trained ImageNet-1K\n",
    "    # https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth\n",
    "    model = SwinTransformer(in_chans=3,\n",
    "                            patch_size=4,\n",
    "                            window_size=7,\n",
    "                            embed_dim=96,\n",
    "                            depths=(2, 2, 18, 2),\n",
    "                            num_heads=(3, 6, 12, 24),\n",
    "                            num_classes=num_classes,\n",
    "                            **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def swin_base_patch4_window7_224(num_classes: int = 1000, **kwargs):\n",
    "    # trained ImageNet-1K\n",
    "    # https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224.pth\n",
    "    model = SwinTransformer(in_chans=3,\n",
    "                            patch_size=4,\n",
    "                            window_size=7,\n",
    "                            embed_dim=128,\n",
    "                            depths=(2, 2, 18, 2),\n",
    "                            num_heads=(4, 8, 16, 32),\n",
    "                            num_classes=num_classes,\n",
    "                            **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def swin_base_patch4_window12_384(num_classes: int = 1000, **kwargs):\n",
    "    # trained ImageNet-1K\n",
    "    # https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384.pth\n",
    "    model = SwinTransformer(in_chans=3,\n",
    "                            patch_size=4,\n",
    "                            window_size=12,\n",
    "                            embed_dim=128,\n",
    "                            depths=(2, 2, 18, 2),\n",
    "                            num_heads=(4, 8, 16, 32),\n",
    "                            num_classes=num_classes,\n",
    "                            **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def swin_base_patch4_window7_224_in22k(num_classes: int = 21841, **kwargs):\n",
    "    # trained ImageNet-22K\n",
    "    # https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22k.pth\n",
    "    model = SwinTransformer(in_chans=3,\n",
    "                            patch_size=4,\n",
    "                            window_size=7,\n",
    "                            embed_dim=128,\n",
    "                            depths=(2, 2, 18, 2),\n",
    "                            num_heads=(4, 8, 16, 32),\n",
    "                            num_classes=num_classes,\n",
    "                            **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def swin_base_patch4_window12_384_in22k(num_classes: int = 21841, **kwargs):\n",
    "    # trained ImageNet-22K\n",
    "    # https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22k.pth\n",
    "    model = SwinTransformer(in_chans=3,\n",
    "                            patch_size=4,\n",
    "                            window_size=12,\n",
    "                            embed_dim=128,\n",
    "                            depths=(2, 2, 18, 2),\n",
    "                            num_heads=(4, 8, 16, 32),\n",
    "                            num_classes=num_classes,\n",
    "                            **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def swin_large_patch4_window7_224_in22k(num_classes: int = 21841, **kwargs):\n",
    "    # trained ImageNet-22K\n",
    "    # https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22k.pth\n",
    "    model = SwinTransformer(in_chans=3,\n",
    "                            patch_size=4,\n",
    "                            window_size=7,\n",
    "                            embed_dim=192,\n",
    "                            depths=(2, 2, 18, 2),\n",
    "                            num_heads=(6, 12, 24, 48),\n",
    "                            num_classes=num_classes,\n",
    "                            **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def swin_large_patch4_window12_384_in22k(num_classes: int = 21841, **kwargs):\n",
    "    # trained ImageNet-22K\n",
    "    # https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth\n",
    "    model = SwinTransformer(in_chans=3,\n",
    "                            patch_size=4,\n",
    "                            window_size=12,\n",
    "                            embed_dim=192,\n",
    "                            depths=(2, 2, 18, 2),\n",
    "                            num_heads=(6, 12, 24, 48),\n",
    "                            num_classes=num_classes,\n",
    "                            **kwargs)\n",
    "    return model\n",
    "model = swin_tiny_patch4_window7_224(num_classes=num_classes).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train()\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    accu_loss = torch.zeros(1).to(device)  # 累计损失\n",
    "    accu_num = torch.zeros(1).to(device)   # 累计预测正确的样本数\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    sample_num = 0\n",
    "    data_loader = tqdm(data_loader, file=sys.stdout)\n",
    "    for step, data in enumerate(data_loader):\n",
    "        images, labels = data\n",
    "        sample_num += images.shape[0]\n",
    "\n",
    "        pred = model(images.to(device))\n",
    "        pred_classes = torch.max(pred, dim=1)[1]\n",
    "        accu_num += torch.eq(pred_classes, labels.to(device)).sum()\n",
    "\n",
    "        loss = loss_function(pred, labels.to(device))\n",
    "        loss.backward()\n",
    "        accu_loss += loss.detach()\n",
    "\n",
    "        data_loader.desc = \"[train epoch {}] loss: {:.3f}, acc: {:.3f}\".format(epoch,\n",
    "                                                                               accu_loss.item() / (step + 1),\n",
    "                                                                               accu_num.item() / sample_num)\n",
    "\n",
    "        if not torch.isfinite(loss):\n",
    "            print('WARNING: non-finite loss, ending training ', loss)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return accu_loss.item() / (step + 1), accu_num.item() / sample_num\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device, epoch):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    accu_num = torch.zeros(1).to(device)   # 累计预测正确的样本数\n",
    "    accu_loss = torch.zeros(1).to(device)  # 累计损失\n",
    "\n",
    "    sample_num = 0\n",
    "    data_loader = tqdm(data_loader, file=sys.stdout)\n",
    "    for step, data in enumerate(data_loader):\n",
    "        images, labels = data\n",
    "        sample_num += images.shape[0]\n",
    "\n",
    "        pred = model(images.to(device))\n",
    "        pred_classes = torch.max(pred, dim=1)[1]\n",
    "        accu_num += torch.eq(pred_classes, labels.to(device)).sum()\n",
    "\n",
    "        loss = loss_function(pred, labels.to(device))\n",
    "        accu_loss += loss\n",
    "\n",
    "        data_loader.desc = \"[valid epoch {}] loss: {:.3f}, acc: {:.3f}\".format(epoch,\n",
    "                                                                               accu_loss.item() / (step + 1),\n",
    "                                                                               accu_num.item() / sample_num)\n",
    "\n",
    "    return accu_loss.item() / (step + 1), accu_num.item() / sample_num"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train epoch 0] loss: 1.945, acc: 0.231:   5%|▍         | 20/417 [01:35<31:29,  4.76s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      2\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdamW(pg, lr\u001B[38;5;241m=\u001B[39mlr, weight_decay\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5E-2\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;66;03m# train\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mdata_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mepoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m# validate\u001B[39;00m\n\u001B[0;32m     12\u001B[0m     val_loss, val_acc \u001B[38;5;241m=\u001B[39m evaluate(model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     13\u001B[0m                                  data_loader\u001B[38;5;241m=\u001B[39mval_loader,\n\u001B[0;32m     14\u001B[0m                                  device\u001B[38;5;241m=\u001B[39mdevice,\n\u001B[0;32m     15\u001B[0m                                  epoch\u001B[38;5;241m=\u001B[39mepoch)\n",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[1;34m(model, optimizer, data_loader, device, epoch)\u001B[0m\n\u001B[0;32m     16\u001B[0m accu_num \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39meq(pred_classes, labels\u001B[38;5;241m.\u001B[39mto(device))\u001B[38;5;241m.\u001B[39msum()\n\u001B[0;32m     18\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_function(pred, labels\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[1;32m---> 19\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m accu_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mdetach()\n\u001B[0;32m     22\u001B[0m data_loader\u001B[38;5;241m.\u001B[39mdesc \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[train epoch \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m] loss: \u001B[39m\u001B[38;5;132;01m{:.3f}\u001B[39;00m\u001B[38;5;124m, acc: \u001B[39m\u001B[38;5;132;01m{:.3f}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(epoch,\n\u001B[0;32m     23\u001B[0m                                                                        accu_loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m/\u001B[39m (step \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m),\n\u001B[0;32m     24\u001B[0m                                                                        accu_num\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m/\u001B[39m sample_num)\n",
      "File \u001B[1;32mD:\\Anconda\\lib\\site-packages\\torch\\_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anconda\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "pg = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.AdamW(pg, lr=lr, weight_decay=5E-2)\n",
    "for epoch in range(epochs):\n",
    "    # train\n",
    "    train_loss, train_acc = train_one_epoch(model=model,\n",
    "                                            optimizer=optimizer,\n",
    "                                            data_loader=train_loader,\n",
    "                                            device=device,\n",
    "                                            epoch=epoch)\n",
    "\n",
    "    # validate\n",
    "    val_loss, val_acc = evaluate(model=model,\n",
    "                                 data_loader=val_loader,\n",
    "                                 device=device,\n",
    "                                 epoch=epoch)\n",
    "\n",
    "    tags = [\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"learning_rate\"]\n",
    "    tb_writer.add_scalar(tags[0], train_loss, epoch)\n",
    "    tb_writer.add_scalar(tags[1], train_acc, epoch)\n",
    "    tb_writer.add_scalar(tags[2], val_loss, epoch)\n",
    "    tb_writer.add_scalar(tags[3], val_acc, epoch)\n",
    "    tb_writer.add_scalar(tags[4], optimizer.param_groups[0][\"lr\"], epoch)\n",
    "\n",
    "    torch.save(model.state_dict(), \"./weights/model-{}.pth\".format(epoch))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}