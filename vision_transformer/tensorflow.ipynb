{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import datetime\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras import Model, layers, initializers\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "assert tf.version.VERSION >= \"2.4.0\",\"version of tf must greater/equal than 2.4.0\"\n",
    "\n",
    "if not os.path.exists(\"./save_weights\"):\n",
    "    os.makedirs(\"./save_weights\")\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 10\n",
    "num_classes = 5\n",
    "freeze_layers = True\n",
    "initial_lr = 0.001\n",
    "weight_decay = 1e-4\n",
    "\n",
    "log_dir = \"./logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"train\"))\n",
    "val_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"val\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 2572 images for training, 1098 images for validation.\n"
     ]
    }
   ],
   "source": [
    "data_root=os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "img_path = './flower_data'\n",
    "#img_path = os.path.join(data_root, \"kaggle/input/flowerdata\", \"flower_data\")\n",
    "\n",
    "train_dir = os.path.join(img_path, \"train\")\n",
    "val_dir = os.path.join(img_path, \"val\")\n",
    "\n",
    "flower_class = [cla for cla in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, cla))]\n",
    "\n",
    "class_dict = dict((value, index) for index, value in enumerate(flower_class))\n",
    "class_indices = dict((k, v) for v,k in enumerate(flower_class))\n",
    "json_str = json.dumps(dict((val, key) for key, val in class_indices.items()), indent=4)\n",
    "with open('class.json', 'w') as json_file:\n",
    "    json_file.write(json_str)\n",
    "\n",
    "train_img_path = glob.glob(train_dir + \"/*/*.jpg\")\n",
    "random.shuffle(train_img_path)\n",
    "train_label_list = [class_dict[path.split(os.path.sep)[-2]] for path in train_img_path]\n",
    "train_num = len(train_img_path)\n",
    "\n",
    "val_img_path = glob.glob(val_dir + \"/*/*.jpg\")\n",
    "random.shuffle(val_img_path)\n",
    "val_label_list = [class_dict[path.split(os.path.sep)[-2]] for path in val_img_path]\n",
    "val_num = len(val_img_path)\n",
    "\n",
    "print(\"using {} images for training, {} images for validation.\".format(train_num, val_num))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def process_train_info(img_path, label):\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize_with_crop_or_pad(image, 224, 224)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = (image / 255. - 0.5) / 0.5\n",
    "    return image, label\n",
    "\n",
    "def process_val_info(img_path, label):\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize_with_crop_or_pad(image, 224, 224)\n",
    "    image = (image / 255. - 0.5) / 0.5\n",
    "    return image, label\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((tf.constant(train_img_path),\n",
    "                                               tf.constant(train_label_list)))\n",
    "total_train = len(train_img_path)\n",
    "train_ds = train_ds.map(process_train_info, num_parallel_calls=AUTOTUNE).cache()\\\n",
    "    .shuffle(buffer_size=total_train).batch(batch_size).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((tf.constant(val_img_path),\n",
    "                                             tf.constant(val_label_list)))\n",
    "val_ds = val_ds.map(process_val_info, num_parallel_calls=AUTOTUNE).cache()\\\n",
    "    .batch(batch_size).prefetch(buffer_size=AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "class PatchEmbed(layers.Layer):\n",
    "    def __init__(self, img_size=224, patch_size=16, embed_dim=768):\n",
    "        super(PatchEmbed, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.img_size = (img_size, img_size)\n",
    "        self.grid_size = (img_size // patch_size, img_size // patch_size)\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "        self.proj = layers.Conv2D(filters=embed_dim, kernel_size=patch_size,\n",
    "                                  strides=patch_size, padding='SAME',\n",
    "                                  kernel_initializer=initializers.LecunNormal(),\n",
    "                                  bias_initializer=initializers.Zeros())\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        B, H, W, C = inputs.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(inputs)\n",
    "        # [B, H, W, C] -> [B, H*W, C]\n",
    "        x = tf.reshape(x, [B, self.num_patches, self.embed_dim])\n",
    "        return x\n",
    "\n",
    "class ConcatClassTokenAddPosEmbed(layers.Layer):\n",
    "    def __init__(self, embed_dim=768, num_patches=196, name=None):\n",
    "        super(ConcatClassTokenAddPosEmbed, self).__init__(name=name)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.cls_token = self.add_weight(name=\"cls\",\n",
    "                                         shape=[1, 1, self.embed_dim],\n",
    "                                         initializer=initializers.Zeros(),\n",
    "                                         trainable=True,\n",
    "                                         dtype=tf.float32)\n",
    "        self.pos_embed = self.add_weight(name=\"pos_embed\",\n",
    "                                         shape=[1, self.num_patches + 1, self.embed_dim],\n",
    "                                         initializer=initializers.RandomNormal(stddev=0.02),\n",
    "                                         trainable=True,\n",
    "                                         dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        batch_size, _, _ = inputs.shape\n",
    "\n",
    "        cls_token = tf.broadcast_to(self.cls_token, shape=[batch_size, 1, self.embed_dim])\n",
    "        x = tf.concat([cls_token, inputs], axis=1)  # [B, 197, 768]\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        return x\n",
    "\n",
    "class Attention(layers.Layer):\n",
    "    k_ini = initializers.GlorotUniform()\n",
    "    b_ini = initializers.Zeros()\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads=8,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 proj_drop_ratio=0.,\n",
    "                 name=None):\n",
    "        super(Attention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias, name=\"qkv\",\n",
    "                                kernel_initializer=self.k_ini, bias_initializer=self.b_ini)\n",
    "        self.attn_drop = layers.Dropout(attn_drop_ratio)\n",
    "        self.proj = layers.Dense(dim, name=\"out\",\n",
    "                                 kernel_initializer=self.k_ini, bias_initializer=self.b_ini)\n",
    "        self.proj_drop = layers.Dropout(proj_drop_ratio)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "\n",
    "        B, N, C = inputs.shape\n",
    "\n",
    "        qkv = self.qkv(inputs)\n",
    "        qkv = tf.reshape(qkv, [B, N, 3, self.num_heads, C // self.num_heads])\n",
    "        qkv = tf.transpose(qkv, [2, 0, 3, 1, 4])\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = tf.matmul(a=q, b=k, transpose_b=True) * self.scale\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        attn = self.attn_drop(attn, training=training)\n",
    "\n",
    "        x = tf.matmul(attn, v)\n",
    "        x = tf.transpose(x, [0, 2, 1, 3])\n",
    "        x = tf.reshape(x, [B, N, C])\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x, training=training)\n",
    "        return x\n",
    "\n",
    "class MLP(layers.Layer):\n",
    "    k_ini = initializers.GlorotUniform()\n",
    "    b_ini = initializers.RandomNormal(stddev=1e-6)\n",
    "\n",
    "    def __init__(self, in_features, mlp_ratio=4.0, drop=0., name=None):\n",
    "        super(MLP, self).__init__(name=name)\n",
    "        self.fc1 = layers.Dense(int(in_features * mlp_ratio), name=\"Dense_0\",\n",
    "                                kernel_initializer=self.k_ini, bias_initializer=self.b_ini)\n",
    "        self.act = layers.Activation(\"gelu\")\n",
    "        self.fc2 = layers.Dense(in_features, name=\"Dense_1\",\n",
    "                                kernel_initializer=self.k_ini, bias_initializer=self.b_ini)\n",
    "        self.drop = layers.Dropout(drop)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x, training=training)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x, training=training)\n",
    "        return x\n",
    "\n",
    "class Block(layers.Layer):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads=8,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 drop_ratio=0.,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 drop_path_ratio=0.,\n",
    "                 name=None):\n",
    "        super(Block, self).__init__(name=name)\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6, name=\"LayerNorm_0\")\n",
    "        self.attn = Attention(dim, num_heads=num_heads,\n",
    "                              qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                              attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio,\n",
    "                              name=\"MultiHeadAttention\")\n",
    "\n",
    "        self.drop_path = layers.Dropout(rate=drop_path_ratio, noise_shape=(None, 1, 1)) if drop_path_ratio > 0. \\\n",
    "            else layers.Activation(\"linear\")\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6, name=\"LayerNorm_1\")\n",
    "        self.mlp = MLP(dim, drop=drop_ratio, name=\"MlpBlock\")\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs + self.drop_path(self.attn(self.norm1(inputs)), training=training)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)), training=training)\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(Model):\n",
    "    def __init__(self, img_size=224, patch_size=16, embed_dim=768,\n",
    "                 depth=12, num_heads=12, qkv_bias=True, qk_scale=None,\n",
    "                 drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0.,\n",
    "                 representation_size=None, num_classes=1000, name=\"ViT-B/16\"):\n",
    "        super(VisionTransformer, self).__init__(name=name)\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.depth = depth\n",
    "        self.qkv_bias = qkv_bias\n",
    "\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.cls_token_pos_embed = ConcatClassTokenAddPosEmbed(embed_dim=embed_dim,\n",
    "                                                               num_patches=num_patches,\n",
    "                                                               name=\"cls_pos\")\n",
    "\n",
    "        self.pos_drop = layers.Dropout(drop_ratio)\n",
    "\n",
    "        dpr = np.linspace(0., drop_path_ratio, depth)\n",
    "        self.blocks = [Block(dim=embed_dim, num_heads=num_heads, qkv_bias=qkv_bias,\n",
    "                             qk_scale=qk_scale, drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio,\n",
    "                             drop_path_ratio=dpr[i], name=\"encoderblock_{}\".format(i))\n",
    "                       for i in range(depth)]\n",
    "\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-6, name=\"encoder_norm\")\n",
    "\n",
    "        if representation_size:\n",
    "            self.has_logits = True\n",
    "            self.pre_logits = layers.Dense(representation_size, activation=\"tanh\", name=\"pre_logits\")\n",
    "        else:\n",
    "            self.has_logits = False\n",
    "            self.pre_logits = layers.Activation(\"linear\")\n",
    "\n",
    "        self.head = layers.Dense(num_classes, name=\"head\", kernel_initializer=initializers.Zeros())\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.patch_embed(inputs)\n",
    "        x = self.cls_token_pos_embed(x)\n",
    "        x = self.pos_drop(x, training=training)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training=training)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.pre_logits(x[:, 0])\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def vit_base_patch16_224_in21k(num_classes: int = 21843, has_logits: bool = True):\n",
    "    model = VisionTransformer(img_size=224,\n",
    "                              patch_size=16,\n",
    "                              embed_dim=768,\n",
    "                              depth=12,\n",
    "                              num_heads=12,\n",
    "                              representation_size=768 if has_logits else None,\n",
    "                              num_classes=num_classes,\n",
    "                              name=\"ViT-B_16\")\n",
    "    return model\n",
    "\n",
    "model = vit_base_patch16_224_in21k(num_classes=num_classes, has_logits=False)\n",
    "model.build((1, 224, 224, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def scheduler(now_epoch):\n",
    "    end_lr_rate = 0.01\n",
    "    rate = ((1 + math.cos(now_epoch * math.pi / epochs)) / 2) * (1 - end_lr_rate) + end_lr_rate\n",
    "    new_lr = rate * initial_lr\n",
    "\n",
    "    with train_writer.as_default():\n",
    "        tf.summary.scalar('learning rate', data=new_lr, step=epoch)\n",
    "    return new_lr\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=initial_lr, momentum=0.9)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_val_acc = 0.\n",
    "for epoch in range(epochs):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "\n",
    "    train_bar = tqdm(train_ds, file=sys.stdout)\n",
    "    for train_images, train_labels in train_bar:\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = model(train_images, training=True)\n",
    "            ce_loss = loss_object(train_labels, output)\n",
    "\n",
    "            matcher = re.compile(\".*(bias|gamma|beta).*\")\n",
    "            l2loss = weight_decay * tf.add_n([\n",
    "                tf.nn.l2_loss(v)\n",
    "                for v in model.trainable_variables\n",
    "                if not matcher.match(v.name)\n",
    "            ])\n",
    "\n",
    "            loss = ce_loss + l2loss\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        train_loss(ce_loss)\n",
    "        train_accuracy(train_labels, output)\n",
    "        train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}, acc:{:.3f}\".format(epoch + 1,\n",
    "                                                                             epochs,\n",
    "                                                                             train_loss.result(),\n",
    "                                                                             train_accuracy.result())\n",
    "\n",
    "    optimizer.learning_rate = scheduler(epoch)\n",
    "\n",
    "    val_bar = tqdm(val_ds, file=sys.stdout)\n",
    "    for val_images, val_labels in val_bar:\n",
    "        output = model(val_images, training=False)\n",
    "        loss = loss_object(val_labels, output)\n",
    "\n",
    "        val_loss(loss)\n",
    "        val_accuracy(val_labels, output)\n",
    "\n",
    "        val_bar.desc = \"valid epoch[{}/{}] loss:{:.3f}, acc:{:.3f}\".format(epoch + 1,\n",
    "                                                                           epochs,\n",
    "                                                                           val_loss.result(),\n",
    "                                                                           val_accuracy.result())\n",
    "\n",
    "    with train_writer.as_default():\n",
    "        tf.summary.scalar(\"loss\", train_loss.result(), epoch)\n",
    "        tf.summary.scalar(\"accuracy\", train_accuracy.result(), epoch)\n",
    "\n",
    "    with val_writer.as_default():\n",
    "            tf.summary.scalar(\"loss\", val_loss.result(), epoch)\n",
    "            tf.summary.scalar(\"accuracy\", val_accuracy.result(), epoch)\n",
    "\n",
    "     # only save best weights\n",
    "    if val_accuracy.result() > best_val_acc:\n",
    "        best_val_acc = val_accuracy.result()\n",
    "        save_name = \"./save_weights/model.ckpt\"\n",
    "        model.save_weights(save_name, save_format=\"tf\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}