{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras import layers, Model\n",
    "from tqdm import tqdm\n",
    "assert tf.version.VERSION >= \"2.4.0\", \"version of tf must greater/equal than 2.4.0\"\n",
    "#print(tf.__path__)\n",
    "print(tf.version.VERSION)\n",
    "if not os.path.exists(\"./save_weights\"):\n",
    "    os.makedirs(\"./save_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "im_height = 224\n",
    "im_width = 224\n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "num_classes = 5\n",
    "\n",
    "log_dir = \"./logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"train\"))\n",
    "val_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"val\"))\n",
    "\n",
    "#data_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "#image_path = os.path.join(data_root, \"kaggle/input/flowerdata\", \"flower_data\")\n",
    "image_path = './flower_data'\n",
    "assert os.path.exists(image_path),\"file:'{}' does not exist\".format(image_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dir = os.path.join(image_path, \"train\")\n",
    "val_dir = os.path.join(image_path, \"val\")\n",
    "\n",
    "random.seed(0)\n",
    "# class dict\n",
    "data_class = [cla for cla in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, cla))]\n",
    "class_num = len(data_class)\n",
    "class_dict = dict((value, index) for index, value in enumerate(data_class))\n",
    "\n",
    "\n",
    "train_image_path = glob.glob(train_dir + \"/*/*.jpg\")\n",
    "random.shuffle(train_image_path)\n",
    "train_num = len(train_image_path)\n",
    "train_label_list = [class_dict[path.split(os.path.sep)[-2]] for path in train_image_path]\n",
    "print(train_label_list[:1])\n",
    "\n",
    "\n",
    "val_image_path = glob.glob(val_dir + \"/*/*.jpg\")\n",
    "random.shuffle(val_image_path)\n",
    "val_num = len(val_image_path)\n",
    "val_label_list = [class_dict[path.split(os.path.sep)[-2]] for path in val_image_path]\n",
    "\n",
    "print(\"using {} images for training, {} images for validation.\".format(train_num, val_num))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "def process_train_img(img_path, label):\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_jpeg(image, channels= 3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize_with_crop_or_pad(image, im_height, im_width)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = (image - mean) / std\n",
    "    return image, label\n",
    "\n",
    "def process_val_img(img_path, label):\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize_with_crop_or_pad(image, im_height, im_width)\n",
    "    image = (image - mean) / std\n",
    "    return image, label\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_image_path,train_label_list))\n",
    "total_train = len(train_image_path)\n",
    "train_dataset = train_dataset.cache().map(process_train_img, num_parallel_calls=AUTOTUNE)\\\n",
    "                             .shuffle(buffer_size= total_train)\\\n",
    "                             .batch(batch_size)\\\n",
    "                             .prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_image_path,val_label_list))\n",
    "total_val = len(val_image_path)\n",
    "val_dataset = val_dataset.cache().map(process_train_img, num_parallel_calls=AUTOTUNE)\\\n",
    "                         .batch(batch_size)\\\n",
    "                         .prefetch(buffer_size=AUTOTUNE)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ConvBNReLU(layers.Layer):\n",
    "    def __init__(self, filters = 1, kernel_size = 1, strides = 1, padding = 'same', **kwargs):\n",
    "        super(ConvBNReLU, self).__init__(**kwargs)\n",
    "\n",
    "        self.conv = layers.Conv2D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  strides=strides,\n",
    "                                  padding=padding,\n",
    "                                  use_bias=False,\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(4e-5),\n",
    "                                  name=\"conv1\")\n",
    "        self.bn = layers.BatchNormalization(momentum=0.9, name=\"bn\")\n",
    "        self.relu = layers.ReLU()\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.bn(x, training=training)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class DWConvBN(layers.Layer):\n",
    "    def __init__(self, kernel_size = 3, strides = 1, padding = 'same', **kwargs):\n",
    "        super(DWConvBN, self).__init__(**kwargs)\n",
    "        self.dw_conv = layers.DepthwiseConv2D(kernel_size=kernel_size,\n",
    "                                              strides=strides,\n",
    "                                              padding=padding,\n",
    "                                              use_bias=False,\n",
    "                                              kernel_regularizer=tf.keras.regularizers.l2(4e-5),\n",
    "                                              name=\"dw1\")\n",
    "\n",
    "        self.bn = layers.BatchNormalization(momentum=0.9, name=\"bn\")\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        x = self.dw_conv(inputs)\n",
    "        x = self.bn(x, training=training)\n",
    "        return x\n",
    "\n",
    "class ChannelShuffle(layers.Layer):\n",
    "    def __init__(self, shape, groups = 2, **kwargs):\n",
    "        super(ChannelShuffle, self).__init__(**kwargs)\n",
    "        batch_size, height, width, num_channels = shape\n",
    "        assert num_channels % 2 == 0\n",
    "        channel_per_group = num_channels // groups\n",
    "\n",
    "        self.reshape1 = layers.Reshape((height, width, groups, channel_per_group))\n",
    "        self.reshape2 = layers.Reshape((height, width, num_channels))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.reshape1(inputs)\n",
    "        x = tf.transpose(x, perm=[0, 1, 2, 4, 3])\n",
    "        x = self.reshape2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChannelSplit(layers.Layer):\n",
    "    def __init__(self, num_splits = 2, **kwargs):\n",
    "        super(ChannelSplit, self).__init__(**kwargs)\n",
    "        self.num_splits = num_splits\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        b1, b2 = tf.split(inputs, num_or_size_splits=self.num_splits, axis=-1)\n",
    "        return b1, b2\n",
    "\n",
    "\n",
    "def shuffle_block_s1(inputs, output_c, stride, prefix):\n",
    "    if stride != 1:\n",
    "        raise ValueError(\"illegal stride value.\")\n",
    "\n",
    "    assert output_c % 2 == 0\n",
    "    branch_c = output_c // 2\n",
    "\n",
    "    print(\"inputs:{} output_c:{} stride:{} prefix:{}\".format(inputs, output_c, stride, prefix))\n",
    "    x1, x2 = ChannelSplit(name=prefix + \"/split\")(inputs)\n",
    "    print(x1)\n",
    "\n",
    "    x2 = ConvBNReLU(filters = branch_c, name = prefix + \"/b2_conv1\")(x2)\n",
    "    x2 = DWConvBN(kernel_size = 3, strides = stride, name = prefix + \"/b2_dw1\")(x2)\n",
    "    x2 = ConvBNReLU(filters=branch_c, name = prefix + \"/b2_conv2\")(x2)\n",
    "\n",
    "    x = layers.Concatenate(name=prefix + \"/concat\")([x1, x2])\n",
    "    x = ChannelShuffle(x.shape, name = prefix + \"/channelshuffle\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def shuffle_block_s2(inputs, output_c: int, stride: int, prefix: str):\n",
    "    if stride != 2:\n",
    "        raise ValueError(\"illegal stride value.\")\n",
    "\n",
    "    assert output_c % 2 == 0\n",
    "    branch_c = output_c // 2\n",
    "\n",
    "    # shortcut branch\n",
    "    x1 = DWConvBN(kernel_size=3, strides=stride, name=prefix + \"/b1_dw1\")(inputs)\n",
    "    x1 = ConvBNReLU(filters=branch_c, name=prefix + \"/b1_conv1\")(x1)\n",
    "\n",
    "    # main branch\n",
    "    x2 = ConvBNReLU(filters=branch_c, name=prefix + \"/b2_conv1\")(inputs)\n",
    "    x2 = DWConvBN(kernel_size=3, strides=stride, name=prefix + \"/b2_dw1\")(x2)\n",
    "    x2 = ConvBNReLU(filters=branch_c, name=prefix + \"/b2_conv2\")(x2)\n",
    "\n",
    "    x = layers.Concatenate(name=prefix + \"/concat\")([x1, x2])\n",
    "    x = ChannelShuffle(x.shape, name=prefix + \"/channelshuffle\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def shufflenet_v2(num_classes: int,\n",
    "                  input_shape: tuple,\n",
    "                  stages_repeats: list,\n",
    "                  stages_out_channels: list):\n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "    if len(stages_repeats) != 3:\n",
    "        raise ValueError(\"expected stages_repeats as list of 3 positive ints\")\n",
    "    if len(stages_out_channels) != 5:\n",
    "        raise ValueError(\"expected stages_out_channels as list of 5 positive ints\")\n",
    "\n",
    "    x = ConvBNReLU(filters = stages_out_channels[0],\n",
    "                   kernel_size=3,\n",
    "                   strides=2,\n",
    "                   name=\"conv1\")(img_input)\n",
    "\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3),\n",
    "                            strides=2,\n",
    "                            padding='same',\n",
    "                            name=\"maxpool\")(x)\n",
    "\n",
    "    stage_name = [\"stage{}\".format(i) for i in [2, 3, 4]]\n",
    "    for name, repeats, output_channels in zip(stage_name,\n",
    "                                              stages_repeats,\n",
    "                                              stages_out_channels[1:]):\n",
    "        for i in range(repeats):\n",
    "            if i == 0:\n",
    "                x = shuffle_block_s2(x, output_c=output_channels, stride=2, prefix=name + \"_{}\".format(i))\n",
    "            else:\n",
    "                x = shuffle_block_s1(x, output_c=output_channels, stride=1, prefix=name + \"_{}\".format(i))\n",
    "\n",
    "    x = ConvBNReLU(filters=stages_out_channels[-1], name=\"conv5\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D(name=\"globalpool\")(x)\n",
    "\n",
    "    x = layers.Dense(units=num_classes, name=\"fc\")(x)\n",
    "    x = layers.Softmax()(x)\n",
    "\n",
    "    model = Model(img_input, x, name=\"ShuffleNetV2_1.0\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def shufflenet_v2_x1_0(num_classes=1000, input_shape=(224, 224, 3)):\n",
    "    # 权重链接: https://pan.baidu.com/s/1M2mp98Si9eT9qT436DcdOw  密码: mhts\n",
    "    model = shufflenet_v2(num_classes=num_classes,\n",
    "                          input_shape=input_shape,\n",
    "                          stages_repeats=[4, 8, 4],\n",
    "                          stages_out_channels=[24, 116, 232, 464, 1024])\n",
    "    return model\n",
    "\n",
    "\n",
    "def shufflenet_v2_x0_5(num_classes=1000, input_shape=(224, 224, 3)):\n",
    "    model = shufflenet_v2(num_classes=num_classes,\n",
    "                          input_shape=input_shape,\n",
    "                          stages_repeats=[4, 8, 4],\n",
    "                          stages_out_channels=[24, 48, 96, 192, 1024])\n",
    "    return model\n",
    "\n",
    "\n",
    "def shufflenet_v2_x2_0(num_classes=1000, input_shape=(224, 224, 3)):\n",
    "    model = shufflenet_v2(num_classes=num_classes,\n",
    "                          input_shape=input_shape,\n",
    "                          stages_repeats=[4, 8, 4],\n",
    "                          stages_out_channels=[24, 244, 488, 976, 2048])\n",
    "    return model\n",
    "\n",
    "model = shufflenet_v2_x1_0(input_shape = (im_height, im_width, 3), num_classes=num_classes)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_weights_path = './shufflenetv2_x1_0.h5'\n",
    "assert os.path.exists(pre_weights_path), \"cannot find {}\".format(pre_weights_path)\n",
    "model.load_weights(pre_weights_path, by_name=True, skip_mismatch=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name = 'train_loss')\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalCrossentropy(name='val_accuracy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_val_acc = 0.\n",
    "\n",
    "# custom learning rate curve\n",
    "def scheduler(now_epoch):\n",
    "    initial_lr = 0.1\n",
    "    end_lr_rate = 0.1  # end_lr = initial_lr * end_lr_rate\n",
    "    rate = ((1 + math.cos(now_epoch * math.pi / epochs)) / 2) * (1 - end_lr_rate) + end_lr_rate\n",
    "\n",
    "    # cosine\n",
    "    new_lr = rate * initial_lr\n",
    "\n",
    "    # writing lr into tensorboard\n",
    "    with train_writer.as_default():\n",
    "        tf.summary.scalar('learning rate', data=new_lr, step=epoch)\n",
    "\n",
    "    return new_lr\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "\n",
    "    train_bar = tqdm(train_dataset, file=sys.stdout)\n",
    "    print(train_bar)\n",
    "    for images, labels in train_bar:\n",
    "        #train_step(images, lables)\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = model(images, training=True)\n",
    "            loss = loss_object(labels, output)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(labels, output)\n",
    "\n",
    "        train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}, acc:{:.3f}\".format(epoch + 1,\n",
    "                                                                             epochs,\n",
    "                                                                             train_loss.result(),\n",
    "                                                                             train_accuracy.result())\n",
    "\n",
    "    optimizer.learning_rate = scheduler(epoch)\n",
    "\n",
    "    val_bar = tqdm(val_dataset, file=sys.stdout)\n",
    "    for images, labels in val_bar:\n",
    "        output = model(images, training=False)\n",
    "        loss = loss_object(labels, output)\n",
    "\n",
    "        val_loss(loss)\n",
    "        val_accuracy(labels, output)\n",
    "\n",
    "        val_bar.desc = \"val epoch[{}/{}] loss:{：3f} acc:{:.3f}\".format(epoch + 1,\n",
    "                                                                       epochs,\n",
    "                                                                       val_loss.result(),\n",
    "                                                                       val_accuracy().result())\n",
    "\n",
    "\n",
    "    # writing training loss and acc\n",
    "    with train_writer.as_default():\n",
    "        tf.summary.scalar(\"loss\", train_loss.result(), epoch)\n",
    "        tf.summary.scalar(\"accuracy\", train_accuracy.result(), epoch)\n",
    "\n",
    "    # writing validation loss and acc\n",
    "    with val_writer.as_default():\n",
    "        tf.summary.scalar(\"loss\", val_loss.result(), epoch)\n",
    "        tf.summary.scalar(\"accuracy\", val_accuracy.result(), epoch)\n",
    "\n",
    "    # only save best weights\n",
    "    if val_accuracy.result() > best_val_acc:\n",
    "        best_val_acc = val_accuracy.result()\n",
    "        model.save_weights(\"./save_weights/shufflenetv2.ckpt\", save_format=\"tf\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    im_height = 224\n",
    "    im_width = 224\n",
    "    num_classes = 5\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    # load image\n",
    "    img_path = \"../tulip.jpg\"\n",
    "    assert os.path.exists(img_path), \"file: '{}' dose not exist.\".format(img_path)\n",
    "    img = Image.open(img_path)\n",
    "    # resize image to 224x224\n",
    "    img = img.resize((im_width, im_height))\n",
    "    plt.imshow(img)\n",
    "\n",
    "    # scaling pixel value to (-1,1)\n",
    "    img = np.array(img).astype(np.float32)\n",
    "    img = (img / 255. - mean) / std\n",
    "\n",
    "    # Add the image to a batch where it's the only member.\n",
    "    img = (np.expand_dims(img, 0))\n",
    "\n",
    "    # read class_indict\n",
    "    json_path = './class_indices.json'\n",
    "    assert os.path.exists(json_path), \"file: '{}' dose not exist.\".format(json_path)\n",
    "\n",
    "    with open(json_path, \"r\") as f:\n",
    "        class_indict = json.load(f)\n",
    "\n",
    "    # create model\n",
    "    model = shufflenet_v2_x1_0(num_classes=num_classes)\n",
    "\n",
    "    weights_path = './save_weights/shufflenetv2.ckpt'\n",
    "    assert len(glob.glob(weights_path+\"*\")), \"cannot find {}\".format(weights_path)\n",
    "    model.load_weights(weights_path)\n",
    "\n",
    "    result = np.squeeze(model.predict(img))\n",
    "    predict_class = np.argmax(result)\n",
    "\n",
    "    print_res = \"class: {}   prob: {:.3}\".format(class_indict[str(predict_class)],\n",
    "                                                 result[predict_class])\n",
    "    plt.title(print_res)\n",
    "    for i in range(len(result)):\n",
    "        print(\"class: {:10}   prob: {:.3}\".format(class_indict[str(i)],\n",
    "                                                  result[i]))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}